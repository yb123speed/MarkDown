# Analyzer

es自带如下分词器

- Standard
- Simple
- Whitespace
- Stop
- Keyword
- Pattern
- Language

## Standard Analyzer

- 默认分词器
- 其组成如图，特性为：
  - 按词切分，支持多语言
  - 小写处理
  - ![Standard Analyzer](https://raw.githubusercontent.com/yb123speed/MarkDown/master/images/elasticsearch/es_analyzer_01.png)(由上至下调用)

## Simple Analyzer

- 其组成如图，特性为：
  - 按照非字母切分
  - 小写处理
  - ![Simple Analyzer](https://raw.githubusercontent.com/yb123speed/MarkDown/master/images/elasticsearch/es_analyzer_02_simple_analyzer.png)
  
## Whitespace Analyzer

- 其组成如图，特性为：
  - 按照空格切分
  - ![Whitespace Analyzer](https://raw.githubusercontent.com/yb123speed/MarkDown/master/images/elasticsearch/es_analyzer_03_whitespace_analyzer.png)

## Stop Analyzer

- Stop Word 指语气助词等修饰性的词语，比如 the、an、的、这等等
- 其组成如图，特性为：
  - 相比Simple Analyzer多了Stop Word处理
  - ![Stop Analyzer](https://raw.githubusercontent.com/yb123speed/MarkDown/master/images/elasticsearch/es_analyzer_04_stop_analyzer.png)

## Keyword Analyzer

- 其组成如图，特性为：
  - 不分词，直接将输入作为一个单词输出
  - ![Keyword Analyzer](https://raw.githubusercontent.com/yb123speed/MarkDown/master/images/elasticsearch/es_analyzer_05_keyword_analyzer.png)
  - **注:当不想给文本做分词时使用，输入什么，输出就什么**

## Pattern Analyzer

- 其组成如图，特性为：
  - 通过正则表达式自定义分割符
  - 默认是\W+，即非字词的符号作为分隔符
  - ![Pattern Analyzer](https://raw.githubusercontent.com/yb123speed/MarkDown/master/images/elasticsearch/es_analyzer_06_pattern_analyzer.png)

## Language Analyzer

- 提供了30+常见语言的分词器
- arabic, armenian, basque,bengali, brazilian, etc...

## 中文分词

- 常用分词系统
  - IK
    - 实现中英文单词的切分，支持ik_smart、ik_maxword等模式
    - 可自定义词库，支持热更新分词词典
    - [Source Code (GitHub)](https://github.com/medcl/elasticsearch-analysis-ik)
  - jieba
    - python中最流行的分词系统，支持分词和词性标注
    - 支持繁体分词、自定义词典、并行分词等
    - [Source Code (GitHub)](https://github.com/sing1ee/elasticsearch-jieba-plugin)
- 基于自然语言处理的分词系统
  - Hanlp
    - 由一系列模型与算法组成的Java工具包，目标是普及自然语言处理在生成中的应用
    - [Source Code (GitHub)](https://github.com/hankcs/HanLP)
  - THULAC
    - THU Lexical Analyzer for Chinese，由清华大学自然语言处理与社会人文实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能
    - [Source Code (GitHub)](https://github.com/microbun/elasticsearch-thulac-plugin)

## 自定义分词

- 当自带的分词无法满足需求时，可以自定义分词
  - 通过自定义Character Filters、Tokenizer和Token Filter实现

### Character Filters

- 在Tokenizer之前对原始文本进行处理，比如增加、删除或者替换字符等
- 自带的如下：
  - HTML Strip 去除html标签和转换html实体
  - Mapping 进行字符替换操作
  - Pattern Replace 进行正则匹配替换
- 会影响后续Tokenizer解析的postion和offset信息

request:

```json
POST _analyze
{
  "tokenizer": "keyword",
  "char_filter": ["html_strip"],
  "text": "<p>I&apos;m so <b>happy</b>!</p>"
}
```

response:

```json
{
    "tokens": [
        {
            "token": "\nI'm so happy!\n",
            "start_offset": 0,
            "end_offset": 32,
            "type": "word",
            "position": 0
        }
    ]
}
```

### Tokenizer

- 将原始文本按照一定规则切分为单词（term or token）
- 自带的如下：
  - standard 按照单词进行分割
  - letter 按照非字符类进行分割
  - whitespace 按照空格进行分割
  - UAX URL Email 按照standard分割，但不会分割邮箱和url
  - NGram 和 Edge NGram 连词分割
  - Path Hierarchy 按照文件路径进行切割

request:

```json
POST _analyze
{
  "tokenizer": "path_hierarchy",
  "text": "/one/two/three"
}
```

response:

```json
{
    "tokens": [
        {
            "token": "/one",
            "start_offset": 0,
            "end_offset": 4,
            "type": "word",
            "position": 0
        },
        {
            "token": "/one/two",
            "start_offset": 0,
            "end_offset": 8,
            "type": "word",
            "position": 0
        },
        {
            "token": "/one/two/three",
            "start_offset": 0,
            "end_offset": 14,
            "type": "word",
            "position": 0
        }
    ]
}
```

### Token Filters

- 对于Tokenizer输出的单词（term）进行增加、删除、修改等操作
- 自带的如下：
  - lowcase 将所有term转换为小写
  - stop 删除 stop words
  - NGram 和 Edge NGram 连词分割
  - Synonym 添加近义词的term

Request:

```json
POST _analyze
{
  "text": "a Hello,world!",
  "tokenizer": "standard",
  "filter": [
    "stop",
    "lowercase",
    {
      "type": "ngram",
      "min_gram": 4,
      "max_gram":4
    }
  ]
}
```

Response:

```json
{
    "tokens": [
        {
            "token": "hell",
            "start_offset": 2,
            "end_offset": 7,
            "type": "<ALPHANUM>",
            "position": 1
        },
        {
            "token": "ello",
            "start_offset": 2,
            "end_offset": 7,
            "type": "<ALPHANUM>",
            "position": 1
        },
        {
            "token": "worl",
            "start_offset": 8,
            "end_offset": 13,
            "type": "<ALPHANUM>",
            "position": 2
        },
        {
            "token": "orld",
            "start_offset": 8,
            "end_offset": 13,
            "type": "<ALPHANUM>",
            "position": 2
        }
    ]
}
```

### 自定义分词的API

- 自定义分词需要在索引的配置中设定

request:

```json
PUT test_index
{
  "settings": {
    "analysis": {
      "char_filter":{},
      "tokenizer": {},
      "filter": {},
      "analyzer": {}
    }
  }
}
```

- **注：char_filter、tokenizer、filter、analyzer可以自定义**

- ![DIY Analyze](https://raw.githubusercontent.com/yb123speed/MarkDown/master/images/elasticsearch/es_analyzer_07_diy_analyze.png)

request:

```json
PUT test_index_1
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "char_filter": [
            "html_strip"
          ],
          "filter": [
            "lowercase",
            "asciifolding"
          ]
        }
      }
    }
  }
}
```

response:

```json
{
    "acknowledged": true,
    "shards_acknowledged": true,
    "index": "test_index_1"
}
```

- 实际调用

request:

```json
POST test_index_1/_analyze
{
  "analyzer": "my_custom_analyzer",
  "text": "Is this <b>a box</b>?"
}
```

response:

```json
{
    "tokens": [
        {
            "token": "is",
            "start_offset": 0,
            "end_offset": 2,
            "type": "<ALPHANUM>",
            "position": 0
        },
        {
            "token": "this",
            "start_offset": 3,
            "end_offset": 7,
            "type": "<ALPHANUM>",
            "position": 1
        },
        {
            "token": "a",
            "start_offset": 11,
            "end_offset": 12,
            "type": "<ALPHANUM>",
            "position": 2
        },
        {
            "token": "box",
            "start_offset": 13,
            "end_offset": 20,
            "type": "<ALPHANUM>",
            "position": 3
        }
    ]
}
```

- ![DIY Analyze Sample](https://raw.githubusercontent.com/yb123speed/MarkDown/master/images/elasticsearch/es_analyzer_08_diy_analyze_sample.png)

## 扩展阅读

- [一篇文章总结语言处理中的分词问题](https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&mid=2247486148&amp;idx=1&amp;sn=817027a204650763c1bea3e837d695ea&source=41#wechat_redirect)