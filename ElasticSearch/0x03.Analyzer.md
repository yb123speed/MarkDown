# Analyzer

es自带如下分词器

- Standard
- Simple
- Whitespace
- Stop
- Keyword
- Pattern
- Language

## Standard Analyzer

- 默认分词器
- 其组成如图，特性为：
  - 按词切分，支持多语言
  - 小写处理
  - ![Standard Analyzer](https://raw.githubusercontent.com/yb123speed/MarkDown/master/images/elasticsearch/es_analyzer_01.png)(由上至下调用)

## Simple Analyzer

- 其组成如图，特性为：
  - 按照非字母切分
  - 小写处理
  - ![Simple Analyzer](https://raw.githubusercontent.com/yb123speed/MarkDown/master/images/elasticsearch/es_analyzer_02_simple_analyzer.png)
  
## Whitespace Analyzer

- 其组成如图，特性为：
  - 按照空格切分
  - ![Whitespace Analyzer](https://raw.githubusercontent.com/yb123speed/MarkDown/master/images/elasticsearch/es_analyzer_03_whitespace_analyzer.png)

## Stop Analyzer

- Stop Word 指语气助词等修饰性的词语，比如 the、an、的、这等等
- 其组成如图，特性为：
  - 相比Simple Analyzer多了Stop Word处理
  - ![Stop Analyzer](https://raw.githubusercontent.com/yb123speed/MarkDown/master/images/elasticsearch/es_analyzer_04_stop_analyzer.png)

## Keyword Analyzer

- 其组成如图，特性为：
  - 不分词，直接将输入作为一个单词输出
  - ![Keyword Analyzer](https://raw.githubusercontent.com/yb123speed/MarkDown/master/images/elasticsearch/es_analyzer_05_keyword_analyzer.png)
  - **注:当不想给文本做分词时使用，输入什么，输出就什么**

## Pattern Analyzer

- 其组成如图，特性为：
  - 通过正则表达式自定义分割符
  - 默认是\W+，即非字词的符号作为分隔符
  - ![Pattern Analyzer](https://raw.githubusercontent.com/yb123speed/MarkDown/master/images/elasticsearch/es_analyzer_06_pattern_analyzer.png)

## Language Analyzer

- 提供了30+常见语言的分词器
- arabic, armenian, basque,bengali, brazilian, etc...

## 中文分词

- 常用分词系统
  - IK
    - 实现中英文单词的切分，支持ik_smart、ik_maxword等模式
    - 可自定义词库，支持热更新分词词典
    - [Source Code (GitHub)](https://github.com/medcl/elasticsearch-analysis-ik)
  - jieba
    - python中最流行的分词系统，支持分词和词性标注
    - 支持繁体分词、自定义词典、并行分词等
    - [Source Code (GitHub)](https://github.com/sing1ee/elasticsearch-jieba-plugin)
- 基于自然语言处理的分词系统
  - Hanlp
    - 由一系列模型与算法组成的Java工具包，目标是普及自然语言处理在生成中的应用
    - [Source Code (GitHub)](https://github.com/hankcs/HanLP)
  - THULAC
    - THU Lexical Analyzer for Chinese，由清华大学自然语言处理与社会人文实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能
    - [Source Code (GitHub)](https://github.com/microbun/elasticsearch-thulac-plugin)

## 扩展阅读

- [一篇文章总结语言处理中的分词问题](https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&mid=2247486148&amp;idx=1&amp;sn=817027a204650763c1bea3e837d695ea&source=41#wechat_redirect)